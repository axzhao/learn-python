{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "try:\n",
    "    sc = SparkContext()\n",
    "except ValueError:\n",
    "    sc.stop()\n",
    "    sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 2, 5, 5]\n",
      "['Apple', 'Orange', 'Grape', 'Banana', 'Apple']\n",
      "[4, 2, 3, 6, 6]\n",
      "[1, 2]\n",
      "[1, 5, 2, 3]\n",
      "2\n",
      "[]\n",
      "[3, 1, 2, 5, 5]\n",
      "[(0, [2]), (1, [1, 3, 5, 5])]\n",
      "[3, 1, 2, 5, 5, 5, 6, 2, 7]\n",
      "[5]\n",
      "[1, 2, 3]\n",
      "[(3, 5), (3, 6), (1, 5), (1, 6), (2, 5), (2, 6), (5, 5), (5, 5), (5, 6), (5, 6)]\n",
      "[('c', 1), ('l', 1), ('m', 2), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "intRDD = sc.parallelize([3, 1, 2, 5, 5])\n",
    "stringRDD = sc.parallelize(['Apple', 'Orange', 'Grape', 'Banana', 'Apple'])\n",
    "\n",
    "# RDD转换为python数据类型\n",
    "print(intRDD.collect())\n",
    "print(stringRDD.collect())\n",
    "\n",
    "# 产生另一个RDD，map，filter，distinct\n",
    "print(intRDD.map(lambda x: x+1).collect())\n",
    "print(intRDD.filter(lambda x: x < 3).collect())\n",
    "print(intRDD.distinct().collect())\n",
    "\n",
    "# 划分为多个RDD\n",
    "subRDD = intRDD.randomSplit([0.4, 0.6])\n",
    "print(len(subRDD))\n",
    "print(subRDD[0].collect())\n",
    "print(subRDD[1].collect())\n",
    "\n",
    "# groupBy\n",
    "groupByRDD = intRDD.groupBy(lambda x: x % 2)\n",
    "print(sorted([(x, sorted(y)) for (x, y) in groupByRDD.collect()]))\n",
    "\n",
    "intRDD1 = sc.parallelize([3, 1, 2, 5, 5])\n",
    "intRDD2 = sc.parallelize([5, 6])\n",
    "intRDD3 = sc.parallelize([2, 7])\n",
    "\n",
    "# 集合运算\n",
    "print(intRDD1.union(intRDD2).union(intRDD3).collect())\n",
    "print(intRDD1.intersection(intRDD2).collect())\n",
    "print(intRDD1.subtract(intRDD2).collect())\n",
    "print(intRDD1.cartesian(intRDD2).collect())\n",
    "\n",
    "# 一个JOB\n",
    "strRDD = sc.parallelize([\"cat\", \"dog\", \"lion\", \"monkey\", \"mouse\"])\n",
    "# 第一个 Map 操作将 RDD 里的各个元素进行映射, RDD 的各个数据元素之间不存在依赖,可以在集群的各个内存中独立计算,也就是并行化\n",
    "rdd1 = strRDD.map(lambda x: (x[0], x))\n",
    "# 第二个 groupby 之后的 Map 操作,为了计算相同 key 下的元素个数,需要把相同 key 的元素聚集到同一个 partition 下,所以造成了数据在内存中的重新分布,即 shuffle 操作.\n",
    "# shuffle 操作是 spark 中最耗时的操作,应尽量避免不必要的 shuffle.\n",
    "rdd2 = rdd1.groupBy(lambda x: x[0]).map(lambda x: (x[0], len(x[1])))\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动作运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[3, 1]\n",
      "[1, 2, 3]\n",
      "[5, 5, 3]\n",
      "(count: 5, mean: 3.2, stdev: 1.6, max: 5.0, min: 1.0)\n",
      "1\n",
      "5\n",
      "1.6\n",
      "5\n",
      "16\n",
      "3.2\n"
     ]
    }
   ],
   "source": [
    "# Actions运算，会马上执行\n",
    "print(intRDD.first())\n",
    "print(intRDD.take(2))\n",
    "print(intRDD.takeOrdered(3))\n",
    "print(intRDD.takeOrdered(3,lambda x:-x))\n",
    "\n",
    "# 统计功能\n",
    "print(intRDD.stats())  # 统计\n",
    "print(intRDD.min())\n",
    "print(intRDD.max())\n",
    "print(intRDD.stdev())  # 标准差\n",
    "print(intRDD.count())\n",
    "print(intRDD.sum())\n",
    "print(intRDD.mean())  # 平均\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### key-value 转换元算\n",
    "虽然RDD中是以键值对形式存在，但是本质上还是一个二元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 5, 1]\n",
      "[4, 6, 6, 2]\n",
      "[(3, 4), (3, 6), (1, 2)]\n",
      "[(3, 4), (1, 2)]\n",
      "[(3, 16), (3, 36), (5, 36), (1, 4)]\n",
      "[(1, 2), (3, 4), (3, 6), (5, 6)]\n",
      "[(1, 2), (3, 4), (3, 6), (5, 6)]\n",
      "[(5, 6), (3, 4), (3, 6), (1, 2)]\n",
      "[(5, 6), (1, 2), (3, 10)]\n",
      "[(3, (4, 8)), (3, (6, 8))]\n",
      "[(1, (2, None)), (3, (4, 8)), (3, (6, 8)), (5, (6, None))]\n",
      "[(3, (4, 8)), (3, (6, 8))]\n",
      "[(1, 2), (5, 6)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kvRDD1 = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2)])\n",
    "print(kvRDD1.keys().collect())\n",
    "print(kvRDD1.values().collect())\n",
    "print(kvRDD1.filter(lambda x: x[0] < 5).collect())\n",
    "print(kvRDD1.filter(lambda x: x[1] < 5).collect())\n",
    "print(kvRDD1.mapValues(lambda x: x**2).collect())\n",
    "print(kvRDD1.sortByKey().collect())\n",
    "print(kvRDD1.sortByKey(True).collect())\n",
    "print(kvRDD1.sortByKey(False).collect())\n",
    "print(kvRDD1.reduceByKey(lambda x, y: x+y).collect())  # 对具有相同key值的数据进行合并\n",
    "\n",
    "kvRDD1 = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2)])\n",
    "kvRDD2 = sc.parallelize([(3, 8)])\n",
    "print(kvRDD1.join(kvRDD2).collect())\n",
    "print(kvRDD1.leftOuterJoin(kvRDD2).collect())\n",
    "print(kvRDD1.rightOuterJoin(kvRDD2).collect())\n",
    "print(kvRDD1.subtractByKey(kvRDD2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### key-value 动作运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n",
      "[(3, 4), (3, 6)]\n",
      "3\n",
      "4\n",
      "defaultdict(<class 'int'>, {3: 2, 5: 1, 1: 1})\n",
      "[4, 6]\n"
     ]
    }
   ],
   "source": [
    "kvRDD1 = sc.parallelize([(3, 4), (3, 6), (5, 6), (1, 2)])\n",
    "kvRDD2 = sc.parallelize([(3, 8)])\n",
    "\n",
    "print(kvRDD1.first())\n",
    "print(kvRDD1.take(2))\n",
    "print(kvRDD1.first()[0])\n",
    "print(kvRDD1.first()[1])\n",
    "print(kvRDD1.countByKey())\n",
    "print(kvRDD1.lookup(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 持久化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvRDD1.persist()\n",
    "kvRDD1.unpersist()\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "??StorageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF(User-defined functions, UDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
