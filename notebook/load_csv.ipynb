{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A   B     C      D  ID\n",
      "0  a  a1   NaN   a1-1   1\n",
      "1  a  a2   NaN   a2-1   2\n",
      "2  a  a2   NaN   a2-2   3\n",
      "3  b  b1  b1-1  end-1   4\n",
      "4  b  b2  b2-1  end-2   5\n"
     ]
    }
   ],
   "source": [
    "# 含有大单元格的数据\n",
    "# 如果没有D级代表最后分类，使用C作为最后分类\n",
    "\n",
    "d = {\"A\": [\"a\", np.nan, np.nan, \"b\", np.nan],\n",
    "     \"B\": [\"a1\", \"a2\", np.nan, \"b1\", \"b2\"],\n",
    "     \"C\": [\"a1-1\", \"a2-1\", \"a2-2\", \"b1-1\", \"b2-1\"],\n",
    "     \"D\": [np.nan, np.nan, np.nan, \"end-1\", \"end-2\"]}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "df[[\"A\", \"B\"]] = df[[\"A\", \"B\"]].ffill()\n",
    "c = np.where(pd.isnull(df[\"D\"]), df[\"D\"], df[\"C\"])\n",
    "d = np.where(pd.isnull(df[\"D\"]), df[\"C\"], df[\"D\"])\n",
    "df[\"C\"] = c\n",
    "df[\"D\"] = d\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "df[\"ID\"] = df.index+1\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B source\n",
      "0  a  a   haha\n",
      "1         haha\n",
      "2  c      haha\n"
     ]
    }
   ],
   "source": [
    "# 将A列值为a的单元格扩充到b列\n",
    "\n",
    "d = {\"A\": [\" a \", \" \", \" c \"],\n",
    "     \"B\": [\"\", \"\", \"\"]}\n",
    "df = pd.DataFrame(d)\n",
    "df = df.apply(lambda x: x.str.strip())\n",
    "df[\"source\"] = \"haha\"\n",
    "df[\"B\"] = df[\"A\"].apply(lambda x: x if x == \"a\" else \"\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+-----+-----+\n",
      "| id| c1| c2| c3| p4|d1_id|d2_id|\n",
      "+---+---+---+---+---+-----+-----+\n",
      "|  1|  c| cc|ccc| p2|    3|    2|\n",
      "|  2|  b| bb|bbb| p1|    2|    1|\n",
      "|  3|  a| aa|aaa| p1|    1|    1|\n",
      "+---+---+---+---+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 为spark的dataFrame添加id\n",
    "# c1, c2, c3 found in df1, p4 found in df2, connect df1 and df2 make df3\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = HiveContext(sc)\n",
    "\n",
    "df1 = sqlContext.createDataFrame(\n",
    "    [(1, \"a\", \"aa\", \"aaa\"), (2, \"b\", \"bb\", \"bbb\"),\n",
    "     (3, \"c\", \"cc\", \"ccc\"), (4, \"d\", \"dd\", \"ddd\")],\n",
    "    (\"id\", \"c1\", \"c2\", \"c3\"))\n",
    "\n",
    "df2 = sqlContext.createDataFrame(\n",
    "    [(1, \"1\", \"11\", \"111\", \"p1\"), (2, \"2\", \"22\", \"\", \"p2\"), (3, \"3\", \"33\", \"\", \"p3\")],\n",
    "    (\"id\", \"p1\", \"p2\", \"p3\", \"p4\"))\n",
    "\n",
    "df3 = sqlContext.createDataFrame(\n",
    "    [(\"a\", \"aa\", \"aaa\", \"p1\", 0, 0), (\"b\", \"bb\", \"bbb\", \"p1\", 0, 0),\n",
    "     (\"z\", \"zz\", \"\", \"p9\", 0, 0), (\"c\", \"cc\", \"ccc\", \"p2\", 0, 0)],\n",
    "    (\"c1\", \"c2\", \"c3\", \"p4\", \"d1_id\", \"d2_id\"))\n",
    "\n",
    "df = df3.\\\n",
    "    join(df1, (df3.c1 == df1.c1) & (df3.c2 == df1.c2) & (df3.c3 == df1.c3), \"inner\").\\\n",
    "    join(df2, (df3.p4 == df2.p4), \"inner\").\\\n",
    "    select(lit(0).alias('id'), df3.c1, df3.c2, df3.c3, df3.p4, df1.id.alias(\"d1_id\"), df2.id.alias(\"d2_id\")).\\\n",
    "    filter((df1.id > 0) & (df2.id > 0))\n",
    "df = df.withColumn(\"id\", row_number().over(\n",
    "    Window.partitionBy(\"id\").orderBy(\"id\")))\n",
    "df.show()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "old = np.array([[1,1,1], [1,1,1]])\n",
    "new = old\n",
    "new[0,:2]=0\n",
    "print(old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
